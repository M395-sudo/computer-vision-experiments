{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo Vision Pipeline\n",
    "\n",
    "In this example, a complete stereo vision pipeline after calibration is implemented.\n",
    "\n",
    "![](pipeline.png)\n",
    "\n",
    "From the dataset SceneNN [1], two images had been captured with two cameras that are not well aligned. Hence, a rectification has to preprocess the data. For the sake of simplicity, assume that the camera orientataions of $\\mathbf{C}_\\text{left}$ and $\\mathbf{C}_\\text{right}$ only differ in a rotation around their z-axes.\n",
    "After rectification, the *Semi Global Block Matching* algorithm of [2] is used in order to find the point correspondences. The result, the disparity map, has to be transfered to the depth map ($z$ map).\n",
    "Finally, a point cloud will be generated using the depth information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some libraries we will need later on...\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper methods. `rot_z` builds up a 2D rotation homography for rotating an image around the origin. The method `calc_r_rect` builds up the rectification homography. For more details on `calc_r_rect`, please consider the slides of Chapter 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_z(angle):\n",
    "    return np.array([\n",
    "            [np.cos(angle), -np.sin(angle), 0],\n",
    "            [np.sin(angle),  np.cos(angle), 0],\n",
    "            [            0,              0, 1]], dtype=np.float)\n",
    "\n",
    "def calc_r_rect(R_left, R_right, C_left, C_right):\n",
    "    # r_0_rect: first row of the rectification rotation matrix\n",
    "    r_0_rect = C_right - C_left\n",
    "    r_0_rect = r_0_rect / np.linalg.norm(r_0_rect) # R is othonormal -> r_0_rect must have unit length!\n",
    "    r_0_rect = r_0_rect.T # row vector\n",
    "    \n",
    "    # k: auxiliary vector\n",
    "    r_2_l = R_left[2:3, :]\n",
    "    r_2_r = R_right[2:3, :]\n",
    "    k = (r_2_l + r_2_r) / 2\n",
    "    k = k.T\n",
    "\n",
    "    # r_1_rect: second row of the rectification rotation matrix\n",
    "    # r_1_rect is perpendicular to k and r_0_rect.\n",
    "    r_1_rect = np.cross(k, r_0_rect.T, axis=0)\n",
    "    r_1_rect = r_1_rect.T # row vector\n",
    "\n",
    "    # r_2_rect: third row of the recitfication rotation matrix\n",
    "    # r_2_rect is perpendicular to r_0_rect and r_1_rect.\n",
    "    r_2_rect = np.cross(r_0_rect.T, r_1_rect.T, axis=0)\n",
    "    r_2_rect = r_2_rect.T # row vector\n",
    "\n",
    "    # build up the recitfication rotation matrix\n",
    "    R_rect = np.vstack((r_0_rect, r_1_rect, r_2_rect))\n",
    "    return R_rect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make also sure that you have saved the input images `C0_unrect.png` and `C1_unrect.png` in the same folder as the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left  = cv2.imread(\"c0_unrect.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img_right = cv2.imread(\"c1_unrect.png\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration results\n",
    "\n",
    "Two images have been recorded with with two cameras. The intrinsic and extrinsic parameters are given below and can be usually determined by a calibration procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrinsics\n",
    "alpha = +5.0 / 180.0 * np.pi # z rotation left camera in radians\n",
    "beta  = -3.0 / 180.0 * np.pi # z rotation right camera in radians\n",
    "R_left = rot_z(alpha) # build up rotation matrix of left camera\n",
    "R_right = rot_z(beta) # build up rotation matrix of right camera\n",
    "C_left  = np.array([[  0, 0, 0]]).T # position of left camera (Assume CCS of left camera is congruent with WCS)\n",
    "C_right = np.array([[0.3, 0, 0]]).T # position of right camera (baseline 30 cm = 0.3 m)\n",
    "baseline = np.linalg.norm(C_right - C_left) # length of baseline: distance between the cameras\n",
    "\n",
    "# instrinsics\n",
    "focal_length = 1454.922678357857 # [px]\n",
    "f_x = focal_length\n",
    "f_y = focal_length\n",
    "skew = 0\n",
    "width = img_left.shape[1]\n",
    "height = img_left.shape[0]\n",
    "assert img_left.shape == img_right.shape # both images should have the same shape\n",
    "c_x = (width - 1) / 2  # image center: x\n",
    "c_y = (height - 1) / 2 # image center: y\n",
    "K = np.array([\n",
    "    [f_x, skew, c_x],\n",
    "    [  0,  f_y, c_y],\n",
    "    [  0,    0,   1]\n",
    "], dtype=np.float)     # our intrinic camera matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectification\n",
    "\n",
    "The rectification transforms the images in such a way that the epipolar lines match. Please consider also the lecture sides of Chapter 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_left_inv = np.linalg.inv(R_left)    # R_left^-1\n",
    "R_right_inv = np.linalg.inv(R_right)  # R_right^-1\n",
    "K_inv = np.linalg.inv(K)              # K^-1\n",
    "\n",
    "R_rect = calc_r_rect(R_left, R_right, C_left, C_right)\n",
    "H_left_rect  = K.dot(R_rect.dot(R_left_inv.dot(K_inv)))   # rectification homography for left camera\n",
    "H_right_rect = K.dot(R_rect.dot(R_right_inv.dot(K_inv)))  # rectification homography for right camera\n",
    "\n",
    "# We can apply homographies on images (instead of point sets) with OpenCV's warpPerspective method...\n",
    "img_left_rect  = cv2.warpPerspective(img_left,  H_left_rect,  (width, height))\n",
    "img_right_rect = cv2.warpPerspective(img_right, H_right_rect, (width, height))\n",
    "\n",
    "# annotate images (not important for exam)\n",
    "img_left_vis  = np.copy(img_left)[::5, ::5] # downsample only for visualization (images are too big for screen)\n",
    "img_right_vis = np.copy(img_right)[::5, ::5]\n",
    "img_left_rect_vis  = np.copy(img_left_rect)[::5, ::5]\n",
    "img_right_rect_vis = np.copy(img_right_rect)[::5, ::5]\n",
    "\n",
    "# put text not important for exam\n",
    "img_left_vis  = cv2.UMat.get(cv2.putText(img_left_vis, 'left',   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "img_right_vis = cv2.UMat.get(cv2.putText(img_right_vis, 'right', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "img_left_rect_vis  = cv2.UMat.get(cv2.putText(img_left_rect_vis, 'left rect.',   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "img_right_rect_vis = cv2.UMat.get(cv2.putText(img_right_rect_vis, 'right rect.', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "\n",
    "\n",
    "imgs_stacked = np.hstack((img_left_vis, img_right_vis))\n",
    "imgs_rect_stacked = np.hstack((img_left_rect_vis, img_right_rect_vis))\n",
    "vis = np.vstack((imgs_stacked, imgs_rect_stacked))\n",
    "\n",
    "cv2.imshow(\"Rectification\", vis)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo correspondence\n",
    "\n",
    "After rectification, we are able to search for pixel correspondences in the left and the right image. For that, we use SGBM [2]. The technique and the parameters of SGBM are not important for the examination in summer term '20 & winter term '20/'21. Feel free to play around with the parameters and look how the 3D result changes at the end..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGBM default parameters from OpenCV's python example: stereo_match.py\n",
    "window_size = 3                # default:   3 \n",
    "min_disp = 170                 # default:  16\n",
    "max_disp = 298                 # default: 112\n",
    "num_disp = max_disp - min_disp # default: 112 - min_disp\n",
    "assert num_disp % 16 == 0, \"Attention: max_disp must be dividable by 16!\"\n",
    "block_size = 21                # default:  16\n",
    "P1 = 8*3*window_size**2        # default:   8*3*window_size**2\n",
    "P2 = 32*3*window_size**2       # default:  32*3*window_size**2\n",
    "disp_left_right_max_diff = 1   # default:   1\n",
    "pre_filter_cap = 0             # default:   0\n",
    "uniq_ratio = 10                # default:  10\n",
    "speckle_win_size = 300         # default: 100\n",
    "speckle_range = 32             # default:  32\n",
    "\n",
    "# helper object that performs the SGBM:\n",
    "stereo = cv2.StereoSGBM_create(minDisparity=min_disp,\n",
    "        numDisparities=num_disp,\n",
    "        blockSize=block_size,\n",
    "        P1=P1,\n",
    "        P2=P2,\n",
    "        disp12MaxDiff=disp_left_right_max_diff,\n",
    "        preFilterCap=pre_filter_cap,\n",
    "        uniquenessRatio=uniq_ratio,\n",
    "        speckleWindowSize=speckle_win_size,\n",
    "        speckleRange=speckle_range)\n",
    "\n",
    "disp_map = stereo.compute(img_left_rect, img_right_rect).astype(np.float32) / 16.0 # the disparity map\n",
    "\n",
    "# removing some invalid disparity values\n",
    "invalid_map_a = (img_left_rect == 0)                        # black borders as an artefact of rectif. -> ignore\n",
    "invalid_map_b = np.zeros_like(invalid_map_a, dtype=np.bool) \n",
    "invalid_map_b[:,:max_disp] = True                           # SGBM has always a invalid left boarder -> ignore\n",
    "invalid_map = np.bitwise_or(invalid_map_a, invalid_map_b)   # merge invalid maps\n",
    "disp_map[invalid_map] = -1                                  # we can set invalid disp. values to neg. values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangulation\n",
    "\n",
    "We can derive the depth map $\\mathbf{Z}$ from the disparity map $\\mathbf{D}$. The relation is given as:  \n",
    "$Z = \\frac{f\\cdot b}{D}$, whereas $f$ and $b$ denote the focal length and the length of the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_map = focal_length * baseline / disp_map\n",
    "depth_map[invalid_map] = -1 # we can set invalid depth values to -1 meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture we know that $\\frac{x_\\text{cam}}{z_\\text{cam}} = \\frac{x_\\text{norm}}{1}$.  \n",
    "Hence, $x_\\text{cam} = x_\\text{norm} \\cdot \\frac{z_\\text{cam}}{1}$\n",
    "and $y_\\text{cam} = y_\\text{norm} \\cdot \\frac{z_\\text{cam}}{1}$.  \n",
    "In order to derive the $\\mathbf{X}_\\text{norm}$, we can undo the affine transformation that usually transforms the $\\mathbf{X}_\\text{norm}$ to $\\mathbf{X}_\\text{img}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up image points (not pixel values) of X_img\n",
    "x_lin_space = np.linspace(0, width - 1, width)    # 0, 1, 2, ..., w-1\n",
    "y_lin_space = np.linspace(0, height - 1, height)  # 0, 1, 2, ..., h-1\n",
    "\n",
    "X_img_x, X_img_y = np.meshgrid(x_lin_space, y_lin_space)\n",
    "\n",
    "# X_img_x:\n",
    "# 0, 1, 2, ..., w-1\n",
    "# 0, 1, 2, ..., w-1\n",
    "\n",
    "# X_img_y:\n",
    "#   0,   0,   0, ...,   0\n",
    "#   1,   1,   1, ...,   1\n",
    "#           ...\n",
    "# h-1, h-1, h-1, ..., h-1\n",
    "\n",
    "X_img_x = X_img_x.reshape((1, width * height)) # flatten the x meshgrid\n",
    "X_img_y = X_img_y.reshape((1, width * height)) # flatten the y meshgrid\n",
    "X_img_w = np.ones_like(X_img_x, dtype=np.float)    # corresponding homogeneous scaling factors for each point\n",
    "X_img = np.vstack((X_img_x, X_img_y, X_img_w)) # stacking to a point matrix\n",
    "\n",
    "# calculating X_norm\n",
    "X_norm = K_inv.dot(X_img)\n",
    "X_norm_inhom = X_norm[:2, :] / X_norm[2, :]\n",
    "\n",
    "# calculating X_cam\n",
    "z_cam = depth_map.reshape((1, width * height))\n",
    "focal_length_of_X_norm = 1.0\n",
    "X_cam_xy_inhom = X_norm_inhom * z_cam\n",
    "X_cam_inhom = np.vstack((X_cam_xy_inhom, z_cam)) # containing x, y and z coordinates od all 3D points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "This section is not important for the examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautifying disparity map\n",
    "disp_map_for_vis = disp_map.astype(np.float64)\n",
    "disp_map_for_vis *= 0.5 # looks more pleasant...\n",
    "disp_map_for_vis[disp_map_for_vis < 0 ] = 0 # draw invalid values with black pixels\n",
    "disp_map_for_vis[disp_map_for_vis > (2**8 - 1) ] = 2**8 - 1 # There's no whiter than white...\n",
    "disp_map_for_vis = disp_map_for_vis.astype(np.uint8)\n",
    "disp_map_for_vis = disp_map_for_vis[::5, ::5]\n",
    "disp_map_for_vis = cv2.UMat.get(cv2.putText(disp_map_for_vis, 'disparity',   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "\n",
    "# beautifying depth map\n",
    "depth_map_for_vis = np.copy(depth_map)\n",
    "depth_map_for_vis *= (2**8 - 1) / np.max(depth_map)\n",
    "depth_map_for_vis[depth_map_for_vis < 0 ] = 0 # draw invalid values with black pixels\n",
    "depth_map_for_vis[depth_map_for_vis > (2**8 - 1) ] = 2**8 - 1 # There's no whiter than white...\n",
    "depth_map_for_vis = depth_map_for_vis.astype(np.uint8)\n",
    "depth_map_for_vis = depth_map_for_vis[::5, ::5]\n",
    "depth_map_for_vis = cv2.UMat.get(cv2.putText(depth_map_for_vis, 'depth',   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0 , (255,), 1))\n",
    "\n",
    "\n",
    "\n",
    "# stacking results to one image\n",
    "disp_and_depth = np.vstack((disp_map_for_vis, depth_map_for_vis))\n",
    "overview = np.hstack((vis, disp_and_depth))\n",
    "\n",
    "# show the results\n",
    "cv2.imshow(\"overview\", overview) # open window\n",
    "cv2.waitKey(0) # wait until any key is pressed\n",
    "cv2.destroyAllWindows() # close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted image consists of the following content:  \n",
    "1st row: raw input images in grayscale  \n",
    "2nd row: rectified images  \n",
    "3rd row: disparity map and depth map  \n",
    "  \n",
    "Optionally: Write down a 3D point cloud. You can open the *Wavefront* file `room.obj` with Meshlab..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write the 3D point cloud\n",
    "\n",
    "colors = img_left_rect.reshape((1, width * height)) # flatten the colors for simpler indexing\n",
    "colors = colors.astype(np.float) / 255.0 # 0 -> 0 & 255 -> 1\n",
    "invalid_points = invalid_map.reshape((1, width * height))\n",
    "\n",
    "print(\"This might take some minutes...\")\n",
    "valid_indices = [i for i in range(width*height) if invalid_points[0, i] == False]\n",
    "with open(\"room.obj\", \"w\") as room:\n",
    "    for i in valid_indices:\n",
    "        x = X_cam_inhom[0, i]\n",
    "        y = X_cam_inhom[1, i]\n",
    "        z = X_cam_inhom[2, i]\n",
    "        gray = colors[0, i]\n",
    "        r = gray\n",
    "        b = gray\n",
    "        g = gray\n",
    "        \n",
    "        room.write(\"v \" + str(x) + \" \" + str(y) + \" \" + str(z) + \" \" + str(r) + \" \" + str(g) + \" \" + str(b) + os.linesep)\n",
    "        if i % 100 == 0 or i == (width * height - 1):\n",
    "            status = (i+1) / (width * height) * 100\n",
    "            print(\"status: %03.2d %%\" % status, end='\\r')\n",
    "    print(\"status: 100 %\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu, and S.-K. Yeung, ‘SceneNN: A Scene Meshes Dataset with aNNotations’, in 2016 Fourth International Conference on 3D Vision (3DV), Stanford, CA, USA, Oct. 2016, pp. 92–101, doi: 10.1109/3DV.2016.18.  \n",
    "[2] H. Hirschmüller, ‘Accurate and Efficient Stereo Processing by Semi-Global Matching and Mutual Information’, in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), San Diego, CA, USA, 2005, vol. 2, pp. 807–814, doi: 10.1109/CVPR.2005.56.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
